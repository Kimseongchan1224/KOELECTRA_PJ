# <div align=center>ğŸ€ë„¤ì´ë²„ ì‡¼í•‘ëª° ë¦¬ë·° ê°ì„± ë¶„ì„ğŸ€</div>

![naver_shop_logo](https://github.com/Kimseongchan1224/KOELECTRA_PJ/assets/79899868/92899dc1-9bcd-458c-be78-2cd2a964e02e)

<div align=center>ë„¤ì´ë²„ ì‡¼í•‘ì€ ë„¤ì´ë²„ ì´ìš©ìì™€ ë„¤ì´ë²„ ì‡¼í•‘ì— ì…ì í•œ ì‡¼í•‘ëª° ë° ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´ ê°„ì˜ í¸ë¦¬í•œ ì—°ê²°ì„ ìœ„í•´ ìƒí’ˆ ê²€ìƒ‰, ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜, ê°€ê²©ë¹„êµ, ì‡¼í•‘ ì»¨í…ì¸  ë“±ì„ ì œê³µí•˜ëŠ” ì‡¼í•‘í¬í„¸ ì„œë¹„ìŠ¤ì´ë‹¤.</div>

>[ì¶œì²˜:ë¡œê³ ](https://www.interad.com/insights/naver-shopping-search-update)&nbsp;/&nbsp;[ì¶œì²˜:ì„¤ëª…](https://m.searchad.naver.com/faq/view/374?from) 

<hr>

## 1.ê°œìš”
ì´ë²ˆ í”„ë¡œì íŠ¸ì—ì„œëŠ” í•œêµ­ì–´ ìì—°ì–´ ì²˜ë¦¬ ëª¨ë¸ì¸ KOELECTRAë¥¼ í™œìš©í•˜ì—¬ ë„¤ì´ë²„ ì‡¼í•‘ëª° ë¦¬ë·°ì˜ ê¸ë¶€ì • ì˜ˆì¸¡ì„ í•˜ê³ ì í•œë‹¤.

### 1-1. ë¬¸ì œì •ì˜
ì˜¨ë¼ì¸ ìƒê¶Œì˜ í™œì„±í™”ë¡œ ì˜¨ë¼ì¸ ì‡¼í•‘ëª°ì˜ ê±°ë˜ê°€ í™œë°œí•´ì§€ê³  ìˆëŠ” ê°€ìš´ë° ì†Œë¹„ìì˜ ì„ íƒì˜ ìˆì–´ ì´ìš©í›„ê¸°ê°€ í¬ê²Œ ì˜í–¥ì„ ë¯¸ì¹˜ê³  ìˆëŠ” ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ë‹¤. ë‹¤ë¥¸ ì´ìš©ìë“¤ì˜ ë°˜ì‘ê³¼ ê²½í—˜ì´ ì†Œë¹„ì ì„ íƒì— ì¤‘ìš”í•œ ì—­í• ì„ í•¨ì— ë”°ë¼ ì´ìš©í›„ê¸°ì˜ ì˜í–¥ë ¥ì´ ì¦ê°€í•˜ë©´ì„œ ì¸í•œ ë¬¸ì œì ë„ ëŠ˜ì–´ë‚˜ê³  ìˆëŠ”ë° ì´ìš©í›„ê¸°ë¥¼ ì¡°ì‘ ë° ì†Œë¹„ìê°€ ì˜¬ë¦° ì´ìš©í›„ê¸°ë¥¼ ì‚­ì œí•˜ëŠ” ë“± ì†Œë¹„ìí”¼í•´ë„ ë°œìƒí•˜ê³  ìˆë‹¤. ì—¬ê¸°ì„œ ë°œìƒí•˜ëŠ” ë¬¸ì œë¥¼ í•´ê²°í•˜ê³ ì KOELECTRAë¥¼ í™œìš©í•˜ì—¬ ë¦¬ë·°ë¥¼ ë¶„ë¥˜í•˜ëŠ” ì‹œìŠ¤í…œì„ ë§Œë“¤ê³ ìí•œë‹¤. <br>
ë˜í•œ 2019ë…„ ì˜¨ë¼ì¸ ë¦¬ë·° ì£¼ìš” í†µê³„ë¥¼ ë³´ë©´ "10ëª… ì¤‘ 6ëª…ì€ Google My Businessì—ì„œ ë¡œì»¬ ë¹„ì¦ˆë‹ˆìŠ¤ì˜ ë¦¬ë·°ë¥¼ ì°¾ì•„ë³¸ë‹¤.", "ê±°ì˜ 10ëª… ì¤‘ 9ëª…ì€ êµ¬ë§¤ ì´ì „ì— ë¡œì»¬ ë¹„ì¦ˆë‹ˆìŠ¤ì— ëŒ€í•œ ë¦¬ë·°ë¥¼ ì½ì–´ ë³¸ë‹¤.", "ì ì¬ ê³ ê°ë“¤ì€ í‰ê· ì ìœ¼ë¡œ 10ê°±ì˜ ì˜¨ë¼ì¸ ë¦¬ë·°ë¥¼ ì½ì–´ë³¸í›„ì— ë¡œì»¬ë¹„ì¦ˆë‹ˆìŠ¤ì— ëŒ€í•´ ì‹ ë¢°ë¥¼ í•˜ê²Œëœë‹¤.", "ì ì¬ ê³ ê° ì¤‘ ì ˆë°˜ì€ ë¦¬ë·°ë¥¼ ì½ì–´ë³¸ í›„ì— ì›¹ì‚¬ì´íŠ¸ë¥¼ ë°©ë¬¸í•˜ë‹¤." ë¼ê³  í†µê³„ê°€ ë³´ì—¬ì£¼ë“¯ì´ ë¦¬ë·°ì˜ ì¤‘ìš”ì„±ì´ ì ì°¨ ê°•ìš”ë˜ê³  ìˆë‹¤.

>[ì¶œì²˜:ì†Œë¹„ìê²½ì œ](https://www.dailycnc.com/news/articleView.html?idxno=209683)&nbsp;/&nbsp;[ì¶œì²˜:ì¸í…”ë¦¬ì‹œìŠ¤í…œ](https://blog.kr.intelisystems.com/2019%EB%85%84%EB%8F%84-%EC%98%A8%EB%9D%BC%EC%9D%B8-%EB%A6%AC%EB%B7%B0-%EC%A3%BC%EC%9A%94-%ED%86%B5%EA%B3%84-top-25/)

### 1.2 ë°ì´í„° ë° ëª¨ë¸ ê°œìš”
| ë°ì´í„°ì…‹ | ê°œìˆ˜ | íŠ¹ì§• |
|----------|---|---|
| ë„¤ì´ë²„ ì‡¼í•‘ëª° í›„ê¸° | 200,000 | ì˜ë¥˜, ì¡í™”, ë¯¸ìš©, ê°€ì „, ì‹í’ˆ ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ ë¬¼ê±´ í¬í•¨.<br>ë¬¼ê±´ í’ˆì§ˆ, ë°°ì†¡ ì†ë„, ì‹¤ë¬¼ê³¼ ì˜¨ë¼ì¸ ê°„ì˜ ê´´ë¦¬ ë“± |

- ì–¸ì–´ : í•œêµ­ì–´
- ì¶œì²˜ : ë„¤ì´ë²„ ì‡¼í•‘
- ìˆ˜ì§‘ê¸°ê°„ : 2020-06 ~ 2020-07
- ì ìˆ˜ : 5ì ë§Œì 

#### ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°
```
5	ë°°ê³µë¹ ë¥´ê³  êµ¿
2	íƒë°°ê°€ ì—‰ë§ì´ë„¤ìš© ì €í¬ì§‘ ë°‘ì—ì¸µì— ë§ë„ì—†ì´ ë†”ë‘ê³ ê°€ê³ 
5	ì•„ì£¼ì¢‹ì•„ìš” ë°”ì§€ ì •ë§ ì¢‹ì•„ì„œ2ê°œ ë” êµ¬ë§¤í–ˆì–´ìš” ì´ê°€ê²©ì— ëŒ€ë°•ì…ë‹ˆë‹¤. ë°”ëŠì§ˆì´ ì¡°ê¸ˆ ì—‰ì„±í•˜ê¸´ í•˜ì§€ë§Œ í¸í•˜ê³  ê°€ì„±ë¹„ ìµœê³ ì˜ˆìš”.
2	ì„ ë¬¼ìš©ìœ¼ë¡œ ë¹¨ë¦¬ ë°›ì•„ì„œ ì „ë‹¬í–ˆì–´ì•¼ í•˜ëŠ” ìƒí’ˆì´ì—ˆëŠ”ë° ë¨¸ê·¸ì»µë§Œ ì™€ì„œ ë‹¹í™©í–ˆìŠµë‹ˆë‹¤. ì „í™”í–ˆë”ë‹ˆ ë°”ë¡œì£¼ì‹ ë‹¤í–ˆì§€ë§Œ ë°°ì†¡ë„ ëˆ„ë½ë˜ì–´ìˆì—ˆë„¤ìš”.. í™•ì¸ì•ˆí•˜ê³  ë°”ë¡œ ì„ ë¬¼í–ˆìœ¼ë©´ í°ì¼ë‚ ë»”í–ˆë„¤ìš”..ì´ë ‡ê²Œ ë°°ì†¡ì´ ì˜¤ë˜ê±¸ë ¸ìœ¼ë©´ ì‚¬ëŠ”ê±° ë‹¤ì‹œ ìƒê°í–ˆì„ê±°ê°™ì•„ìš” ì•„ì‰½ë„¤ìš”..
5	ë¯¼íŠ¸ìƒ‰ìƒ ì˜ˆë»ìš”. ì˜† ì†ì¡ì´ëŠ” ê±°ëŠ” ìš©ë„ë¡œë„ ì‚¬ìš©ë˜ë„¤ìš” ã…ã…
2	ë¹„ì¶”í•©ë‹ˆë‹¤ ê³„ë€ ë’¤ì§‘ì„ ë•Œ ì™„ì „ ë¶ˆí¸í•´ìš” ã… ã…  ì½”íŒ…ë„ ë¬»ì–´ë‚˜ê³  ë³´ê¸°ì—” ì˜ˆì˜ê³  ì‹¤ìš©ì ìœ¼ë¡œ ë³´ì˜€ëŠ”ë° ìƒê°ë³´ë‹¤ ì§„ì§œ ë³„ë¡œì…ë‹ˆë‹¤.
```

>[ì¶œì²˜:git/KiimKii](https://github.com/KiimKii/nsrd)

### 1.3 ê°ì„±ë¶„ì„ ìˆœì„œ

![count](https://github.com/Kimseongchan1224/KOELECTRA_PJ/assets/79899868/904b9fad-aa94-47a7-8156-0912e1efe9a2)

1. ë¶„ì„í•  ë„¤ì´ë²„ ì‡¼í•‘ëª° ë¦¬ë·°ë°ì´í„°ë¥¼ ê¹ƒí—ˆë¸Œì—ì„œ í™•ë³´í•œë‹¤.
2. íŒŒì´ì°¸ì—ì„œ ì¸ì‹í• ìˆ˜ ìˆê²Œ í…ìŠ¤íŠ¸í™” ì‘ì—…ì„ í•œë‹¤.
3. pythonì„ í™œìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ í•™ìŠµì‹œí‚¤ëŠ” ì‘ì—…í•œë‹¤.
4. í•™ìŠµì´ ëœ í”„ë¡œê·¸ë¨ì„ ì‘ë™ì‹œì¼œ ê¸ì •ê³¼ ë¶€ì •ìœ¼ë¡œ ë¶„ë¥˜í•˜ëŠ”ì§€ í™•ì¸ì„ í•œë‹¤.

>[ì¶œì²˜:ë¡œê³ ](https://blog.naver.com/hn03055/222730453539)&nbsp;/&nbsp;[ì¶œì²˜:ë¡œê³ ](http://wiki.hash.kr/index.php/%ED%8C%8C%EC%9D%B4%EC%B0%B8)

## 2. ë°ì´í„°

### 2.1 ë°ì´í„° ì†ŒìŠ¤

#### ì›ì‹œë°ì´í„°
| num | ratings | reviews | label |
|---|----------|---|---|
| 1 | 5 | ë°°ê³µë¹ ë¥´ê³  êµ¿ | 1 |   
| 2 | 2 | íƒë°°ê°€ ì—‰ë§ì´ë„¤ìš© ì €í¬ì§‘ ë°‘ì—ì¸µì— ë§ë„ì—†ì´ ë†”ë‘ê³ ê°€ê³  | 0 |   
| 3 | 5 | ì•„ì£¼ì¢‹ì•„ìš” ë°”ì§€ ì •ë§ ì¢‹ì•„ì„œ2ê°œ ë” êµ¬ë§¤í–ˆì–´ìš” ì´....  | 1 |  
| 4 | 2 | ì„ ë¬¼ìš©ìœ¼ë¡œ ë¹¨ë¦¬ ë°›ì•„ì„œ ì „ë‹¬í–ˆì–´ì•¼ í•˜ëŠ” ìƒí’ˆì´ ì˜€.... | 0 |
|...|...|...|...|
| 199997 | 5 | ë‹¤ì´ìŠ¨ ì¼€ì´ìŠ¤ êµ¬ë§¤í–ˆì–´ìš” ë‹¤ì´ìŠ¨ ìŠˆí¼ì†Œë‹‰ ë“œë¼ì´...| 1 |    
| 199998 | 5 | ë¡œë“œìƒ¾ì—ì„œ ì‚¬ëŠ”ê²ƒë³´ë‹¤ ì„¸ë°° ì €ë ´í•˜ë„¤ìš” ã…œã…œ ìì£¼...| 1 |   
| 199999 | 5 | ë„˜ì´ì˜ê³  ì„ë ¨ë˜ë³´ì´ë„¤ìš”~| 1 |  
| 200000 | 5 | ì•„ì§ ì‚¬ìš©í•´ë³´ì§€ë„ì•Šì•˜ê³  ë‹¤ë¥¸ ì œí’ˆì„ ì¨ë³¸ì ì´ì—†.... | 1 |  
| ë²ˆí˜¸ | ë³„ì  | ê³ ê°ì´ ë‚¨ê¸´ í…ìŠ¤íŠ¸ ë¦¬ë·° | ë¶€ì •(0), ê¸ì •(1) | 

#### ë°ì´í„° ë¶„í¬
<table>
  <tr><th></th><th>ë³„ì </th><th>ê±´ìˆ˜</th></tr>
  <tr><th rowspan='2'>ê¸ì • (99,963)</th><th>5</th><td>81,177</td></tr>
  <tr><th>4</th><td>18,786</td></tr>
  <tr><th rowspan='2'>ë¶€ì • (100,037)</th><th>2</th><td>63,989</td></tr>
  <tr><th>1</th><td>36,048</td></tr>
  <tr><th colspan='2'>ê³„</th><td>200,000</td></tr>
</table>

ë„¤ì´ë²„ ì‡¼í•‘ì—ì„œ ì œí’ˆë³„ í›„ê¸°ë¥¼ ë³„ì ê³¼ í•¨ê»˜ ìˆ˜ì§‘í•œ ê²ƒì´ë‹¤. ë°ì´í„°ëŠ” íƒ­ìœ¼ë¡œ ë¶„ë¦¬ë˜ì–´ ìˆìœ¼ë©°, ì²«ë²ˆì§¸ í•„ë“œì—ëŠ” ë³„ì (1 ~ 5), ë‘ë²ˆì§¸ í•„ë“œì—ëŠ” í…ìŠ¤íŠ¸ê°€ ìœ„ì¹˜í•œë‹¤. ê¸/ë¶€ì •ìœ¼ë¡œ ë¶„ë¥˜í•˜ê¸° ì• ë§¤í•œ 3ì ì— í•´ë‹¹í•˜ëŠ” í…ìŠ¤íŠ¸ë“¤ì€ ì œì™¸í•˜ì˜€ê³ , ê¸ì •(4 ~ 5ì )ê³¼ ë¶€ì •(1 ~ 2ì )ì˜ ë¹„ìœ¨ì´ 1:1ì— ê°€ê¹ë„ë¡ ìƒ˜í”Œë§ì„ í–ˆë‹¤.

### 2.2 íƒìƒ‰ì  ë°ì´í„° ë¶„ì„
#### ë¶„í¬í™•ì¸
```
total_data = pd.read_table('ratings_total.txt', names=['ratings', 'reviews'])
print('ì „ì²´ ë¦¬ë·° ê°œìˆ˜ :',len(total_data)) # ì „ì²´ ë¦¬ë·° ê°œìˆ˜ ì¶œë ¥
```
```
ì „ì²´ ë¦¬ë·° ê°œìˆ˜ : 200000
```
ì´ 20ë§Œê°œì˜ ë°ì´í„° ìƒ˜í”Œì´ ì¡´ì¬í•œë‹¤.
```
total_data['label'].value_counts().plot(kind = 'bar')
```
![cap1](https://github.com/Kimseongchan1224/KOELECTRA_PJ/assets/79899868/f3176b37-a696-4328-a023-edfcf15f1ebd)
```
print(total_data.groupby('label').size().reset_index(name = 'count'))
```
```
   label   count
0      0  100037
1      1   99963
```
0ê³¼ 1 ëª¨ë‘ ì•½ 1ë§Œê°œì˜ ë°ì´í„°ë¡œ 50:50 ë¹„ìœ¨ì„ ê°€ì§€ê³  ìˆë‹¤.
```
total_data['ratings'].value_counts().plot(kind = 'bar')
```
![cap2](https://github.com/Kimseongchan1224/KOELECTRA_PJ/assets/79899868/235e7f15-1367-457e-9800-8e512ef7c2d8)
```
print(total_data.groupby('ratings').size().reset_index(name = 'count'))
```

```
ratings  count
0        1  36048
1        2  63989
2        4  18786
3        5  81177
```
ê³ ê°ì´ ë‚¨ê¸´ ë³„ì  ë°ì´í„°ì˜ ìˆ˜ëŠ” <u>5</u>ì ì´ 81177ê°œ, <u>2</u>ì ì´ 63989ê°œ, <u>1</u>ì ì´ 36048ê°œ, <u>4</u>ì ì´ 18786ê°œì˜ ë°ì´í„°ë¥¼ ë³´ìœ í•˜ê³  ìˆë‹¤.
#### 
### 2.3 ë°ì´í„° ì „ì²˜ë¦¬
#### ì¤‘ë³µ ë¦¬ë·°ë°ì´í„° 
```
print("ì„œë¡œ ë‹¤ë¥¸ ë°ì´í„°ì˜ ìˆ˜ :" , data['reviews'].nunique())
data.drop_duplicates(subset=['reviews'], inplace=True)
print("ë°ì´í„° ê°œìˆ˜ (ì¤‘ë³µì œê±°) : ", len(data))
```
```
ì„œë¡œ ë‹¤ë¥¸ ë°ì´í„°ì˜ ìˆ˜ : 199904
ë°ì´í„° ê°œìˆ˜ (ì¤‘ë³µì œê±°) :  199904
1    99952
0    99952
```
í…ìŠ¤íŠ¸ ë¦¬ë·°ê°€ ì¤‘ë³µì¸ ë°ì´í„°ë¥¼ 96ê°œë¥¼ ì œê±°í•˜ì˜€ë‹¤.
#### ê²°ì¸¡ì¹˜ ì œê±°
```
print(data.isnull().values.any()) #ê²°ì¸¡ì¹˜(Null, NaN)ê°€ ìˆë‹¤ë©´ True
data = data.dropna(how='any')
data['reviews'] = data['reviews'].str.replace("[^ã„±-ã…ã…-ã…£ê°€-í£]","")
print(data[:5])
```
```
   ratings                                            reviews                                                 label
0        5                                             ë°°ê³µë¹ ë¥´ê³ êµ¿                                             1
1        2                           íƒë°°ê°€ì—‰ë§ì´ë„¤ìš©ì €í¬ì§‘ë°‘ì—ì¸µì—ë§ë„ì—†ì´ë†”ë‘ê³ ê°€ê³                               0
2        5  ì•„ì£¼ì¢‹ì•„ìš”ë°”ì§€ì •ë§ì¢‹ì•„ì„œê°œë”êµ¬ë§¤í–ˆì–´ìš”ì´ê°€ê²©ì—ëŒ€ë°•ì…ë‹ˆë‹¤ë°”ëŠì§ˆì´ì¡°ê¸ˆì—‰ì„±í•˜ê¸´í•˜ì§€ë§Œí¸í•˜ê³ ê°€ì„±...            1
3        2  ì„ ë¬¼ìš©ìœ¼ë¡œë¹¨ë¦¬ë°›ì•„ì„œì „ë‹¬í–ˆì–´ì•¼í•˜ëŠ”ìƒí’ˆì´ì—ˆëŠ”ë°ë¨¸ê·¸ì»µë§Œì™€ì„œë‹¹í™©í–ˆìŠµë‹ˆë‹¤ì „í™”í–ˆë”ë‹ˆë°”ë¡œì£¼ì‹ ë‹¤í–ˆ...            0
4        5                          ë¯¼íŠ¸ìƒ‰ìƒì˜ˆë»ìš”ì˜†ì†ì¡ì´ëŠ”ê±°ëŠ”ìš©ë„ë¡œë„ì‚¬ìš©ë˜ë„¤ìš”ã…ã…                             1
```
ê²°ì¸¡ì¹˜ì¸ Null, NaNì˜ ê°’ê³¼ ë„ì–´ì“°ê¸°ê°€ ì œê±° ë˜ì—ˆë‹¤.
## 3. ì¬í•™ìŠµ ê²°ê³¼

### 3.1 ê³„ë°œí™˜ê²½
<img src="https://img.shields.io/badge/Python-3776AB?style=flat-square&logo=Python&logoColor=white"/></a>
<img src="https://img.shields.io/badge/PyTorch-E34F26?style=flatsquare&logo=PyTorch&logoColor=white"/></a>
<img src="https://img.shields.io/badge/PyCharm-000000?style=flat-square&logo=PyCharm&logoColor=white"/></a>
<img src="https://img.shields.io/badge/Jupyter-F37626?style=flat-square&logo=Jupyter&logoColor=white"/></a>

### 3.2  KOELECTRA fine-tuning
```
import pandas as pd
import numpy as np
from transformers import ElectraTokenizer, ElectraForSequenceClassification
from transformers import get_linear_schedule_with_warmup, logging
from sklearn.model_selection import train_test_split
import torch
from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler
import time
import datetime

train_data_path = "review30000.txt"

dataset = pd.read_csv(train_data_path, sep="\t").dropna(axis=0)
text = list(dataset['reviews'].values)
label = dataset['label'].values

# ë°ì´í„° í™•ì¸
num_to_print = 3
print("\n\n*** ë°ì´í„° ***")
for j in range(num_to_print):
    print(f'reviews: {text[j][:20]}, label : {label[j]}')
print(f'\t * í•™ìŠµ ë°ì´í„°ì˜ ìˆ˜ : {len(text)}')
print(f'\t * ë¶€ì • ë¦¬ë·° ìˆ˜ : {list(label).count(0)}')
print(f'\t * ê¸ì • ë¦¬ë·° ìˆ˜ : {list(label).count(1)}')

# í† í°í™”
tokenizer = ElectraTokenizer.from_pretrained("koelectra-small-v3-discriminator")
inputs = tokenizer(text, truncation=True, max_length=256, add_special_tokens=True, padding="max_length")
input_ids = inputs['input_ids']
attention_mask = inputs['attention_mask']
print("\n\n*** í† í°í™” ***")
for j in range(num_to_print):
    print(f'\n{j+1}ë²ˆì§¸ ë°ì´í„°')
    print("** í† í° **")
    print(input_ids[j])
    print("** ì–´í…ì…˜ ë§ˆìŠ¤í¬ **")
    print(attention_mask[j])

train, validation, train_y, validation_y = train_test_split(input_ids, label, test_size=0.2, random_state=2023)
train_masks, validation_masks, _, _ = train_test_split(attention_mask, label, test_size=0.2, random_state=2023)

print("\n\n*** ë°ì´í„° ë¶„ë¦¬ ***")
print(f"í•™ìŠµ ë°ì´í„° ìˆ˜ : {len(train)}")
print(f"ê²€ì¦ ë°ì´í„° ìˆ˜ : {len(validation)}")

batch_size = 32
train_inputs = torch.tensor(train)
train_labels = torch.tensor(train_y)
train_masks  = torch.tensor(train_masks)
train_data   = TensorDataset(train_inputs, train_masks, train_labels)
train_sampler = RandomSampler(train_data)
train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)

validation_inputs = torch.tensor(validation)
validation_labels = torch.tensor(validation_y)
validation_masks  = torch.tensor(validation_masks)
validation_data   = TensorDataset(validation_inputs, validation_masks, validation_labels)
validation_sampler = RandomSampler(validation_data)
validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)

model = ElectraForSequenceClassification.from_pretrained('koelectra-small-v3-discriminator', num_labels=2)

#optimizer = torch.optim.AdamW(model.parameters(), lr=5e-06)
optimizer = torch.optim.Adam(model.parameters(),lr=3e-04, eps=1e-06, betas=(0.9, 0.999))

epoch = 4
scheduler = get_linear_schedule_with_warmup(optimizer,
                                            num_warmup_steps=0,
                                            num_training_steps=len(train_dataloader)*epoch)

for e in range(0, epoch):
    print(f'\n\nEpoch {e+1} of {epoch}')
    print(f'** í•™ìŠµ **')
    t0 = time.time()
    total_loss = 0
    model.train()
    for step, batch in enumerate(train_dataloader):
        if step % 50 == 0 and not step == 0:
            elapsed_rounded = int(round(time.time() - t0))
            elapsed = str(datetime.timedelta(seconds=elapsed_rounded))
            print(f'Batch {step} of {len(train_dataloader)}, ê±¸ë¦° ì‹œê°„ : {elapsed}')

        batch_ids, batch_mask, batch_labels = tuple(t for t in batch)

        model.zero_grad()

        outputs = model(batch_ids, token_type_ids=None, attention_mask=batch_mask, labels=batch_labels)
        loss = outputs.loss
        total_loss += loss.item()

        if step % 10 == 0 and not step == 0:
            print(f'step : {step}, loss : {loss.item()}')

        loss.backward()
        torch.nn.utils.clip_grad_norm(model.parameters(), 1.0)
        optimizer.step()
        scheduler.step()

    avg_train_loss = total_loss / len(train_dataloader)
    print(f"í‰ê·  í•™ìŠµ ì˜¤ì°¨(loss) : {avg_train_loss}")
    epoch_elapsed_time = int(round(time.time() - t0))
    print(f"í•™ìŠµì— ê±¸ë¦° ì‹œê°„ : {str(datetime.timedelta(seconds=epoch_elapsed_time))}")

    print('\n\n** ê²€ì¦ **')
    t0 = time.time()
    model.eval()
    eval_loss, eval_accuracy, eval_steps, eval_example = 0, 0, 0, 0
    for batch in validation_dataloader:
        batch_ids, batch_mask, batch_labels = tuple(t for t in batch)

        with torch.no_grad():
            outputs = model(batch_ids, token_type_ids=None, attention_mask=batch_mask)

        logits = outputs[0]
        logits = logits.numpy()
        label_ids = batch_labels.numpy()

        pred_flat = np.argmax(logits, axis=1).flatten()
        labels_flat = label_ids.flatten()
        eval_accuracy_temp = np.sum(pred_flat == labels_flat) / len(labels_flat)
        eval_accuracy += eval_accuracy_temp
        eval_steps += 1
    print(f"ê²€ì¦ ì •í™•ë„ : {eval_accuracy / eval_steps}")
    val_elapsed_time = int(round(time.time() - t0))
    print(f"ê²€ì¦ì— ê±¸ë¦° ì‹œê°„ : {str(datetime.timedelta(seconds=val_elapsed_time))}")

print("\n\n** ëª¨ë¸ ì €ì¥ **")
save_path = 'koelectra_small'
model.save_pretrained(save_path + ".pt")
print("\n** ë **")
```


